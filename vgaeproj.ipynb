{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torch-geometric\n",
    "%pip install pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Example edge list (source, target)\n",
    "#edge_list = [(0,1),(1,2),(1,5),(2,3),(2,6),(5,6),(4,5),(3,4)]\n",
    "\n",
    "# Create edge_index tensor (positive edge)\n",
    "edge_index_1 = [[0,1,0,3,1,3,1,2,1,4,2,4,2,6,3,5,3,6,4,5,5,6],\n",
    "              [1,0,3,0,3,1,2,1,4,1,4,2,6,2,5,3,6,3,5,4,6,5]]\n",
    "positive_edge_index = torch.tensor(edge_index_1, dtype = torch.long)\n",
    "\n",
    "# Reading and Normalizing Features of steady state data \n",
    "csv_file_path = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\steady_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features = pd.read_csv(csv_file_path, usecols=columns_to_read)\n",
    "\n",
    "features = features.iloc[0:100000].values  # Use the first 20000 instances\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "features1_steady = torch.tensor(features_normalized, dtype=torch.float)  # Use your actual features\n",
    "\n",
    "#readying fault data\n",
    "csv_file_path_1 = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\fault_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features_fault = pd.read_csv(csv_file_path_1, usecols=columns_to_read)\n",
    "\n",
    "features_fault = features_fault.iloc[0:26600].values  # Use the first 20000 instances\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized_fault = scaler.fit_transform(features_fault)\n",
    "features1_fault = torch.tensor(features_normalized_fault, dtype=torch.float)\n",
    "\n",
    "#concatenating fault and steady features\n",
    "features_steady_fault = torch.cat((features1_fault,features1_steady),dim = 0)\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "         self.features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "features_tensor = features_steady_fault\n",
    "dataset = GraphDataset(features_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Transpose the feature matrix to have nodes as rows and instances as columns\n",
    "#features1 = features1.T\n",
    "\n",
    "#num_nodes = features1.size(1)  # Number of nodes\n",
    "#num_features = features1.size(0)  # Number of features (instances)\n",
    "\n",
    "data = Data(\n",
    "    x=features_steady_fault,\n",
    "    edge_index=positive_edge_index\n",
    ")\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(VariationalGCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)  # Apply dropout after the first convolution layer\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "class NodeFeatureDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(NodeFeatureDecoder, self).__init__()\n",
    "        self.lin1 = Linear(in_channels, 2 * out_channels)\n",
    "        self.lin2 = Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.lin1(z))\n",
    "        return self.lin2(z)\n",
    "\n",
    "class VGAEWithNodeFeatureReconstruction(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VGAEWithNodeFeatureReconstruction, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logstd)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        mu, logstd = self.encode(x, edge_index)\n",
    "        z = self.reparameterize(mu, logstd)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logstd\n",
    "\n",
    "# Define model\n",
    "encoder = VariationalGCNEncoder(7, 16, dropout_rate=0.4)\n",
    "decoder = NodeFeatureDecoder(16, 7)\n",
    "model = VGAEWithNodeFeatureReconstruction(encoder, decoder)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\"\"\"\n",
    "\n",
    "##this model initialization had gcn in encoder and linear in decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model has gcn in both encoder and decoder \n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Example edge list (source, target)\n",
    "edge_index_1 = [[0,1,0,3,1,3,1,2,1,4,2,4,2,6,3,5,3,6,4,5,5,6],\n",
    "              [1,0,3,0,3,1,2,1,4,1,4,2,6,2,5,3,6,3,5,4,6,5]]\n",
    "positive_edge_index = torch.tensor(edge_index_1, dtype=torch.long)\n",
    "\n",
    "# Reading and Normalizing Features of steady state data \n",
    "csv_file_path = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\steady_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features = pd.read_csv(csv_file_path, usecols=columns_to_read)\n",
    "\n",
    "features = features.iloc[0:30000].values  # Use the first 100000 instances\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "features1_steady = torch.tensor(features_normalized, dtype=torch.float) \n",
    "\n",
    "# Reading and Normalizing Fault Data\n",
    "#csv_file_path_1 = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\fault_data.csv'\n",
    "#features_fault = pd.read_csv(csv_file_path_1, usecols=columns_to_read)\n",
    "\n",
    "#features_fault = features_fault.iloc[0:26600].values \n",
    "#scaler = MinMaxScaler()\n",
    "#features_normalized_fault = scaler.fit_transform(features_fault)\n",
    "#features1_fault = torch.tensor(features_normalized_fault, dtype=torch.float)\n",
    "\n",
    "# Concatenating fault and steady features\n",
    "#features_steady_fault = torch.cat((features1_fault,features1_steady),dim = 0)\n",
    "\n",
    "#USING STEADY DATA ONLY \n",
    "features_steady_fault = features1_steady\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "features_tensor = features_steady_fault\n",
    "dataset = GraphDataset(features_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "data = Data(\n",
    "    x=features_steady_fault,\n",
    "    edge_index=positive_edge_index\n",
    ")\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(VariationalGCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x) \n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "class NodeFeatureDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(NodeFeatureDecoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        z = F.relu(self.conv1(z, edge_index))\n",
    "        z = self.dropout(z)  \n",
    "        return self.conv2(z, edge_index)\n",
    "\n",
    "class VGAEWithNodeFeatureReconstruction(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VGAEWithNodeFeatureReconstruction, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logstd)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        mu, logstd = self.encode(x, edge_index)\n",
    "        z = self.reparameterize(mu, logstd)\n",
    "        recon_x = self.decode(z, edge_index)\n",
    "        return recon_x, mu, logstd\n",
    "\n",
    "# Define model\n",
    "encoder = VariationalGCNEncoder(7, 16, dropout_rate=0.2)\n",
    "decoder = NodeFeatureDecoder(16, 7, dropout_rate=0.2)\n",
    "model = VGAEWithNodeFeatureReconstruction(encoder, decoder)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with batching\n",
    "epochs = 100\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        data = Data(x=batch, edge_index=positive_edge_index)\n",
    "        recon_x, mu, logstd = model(data.x, data.edge_index)\n",
    "        recon_loss = F.mse_loss(recon_x, data.x)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logstd - mu.pow(2) - logstd.exp())\n",
    "        loss = recon_loss + kl_loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch details\n",
    "        print(f'Epoch: {epoch:03d}, Batch: {batch_idx:03d}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Data being trained (first 5 rows):\\n{data.x[:5]}')\n",
    "        print(f'Reconstructed data (first 5 rows):\\n{recon_x[:5]}')\n",
    "        print(f'Epoch: {epoch:03d}, Recon Loss: {recon_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, Total Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Training loop with batching and data visualization\n",
    "epochs = 2000\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        data = Data(x=batch, edge_index=positive_edge_index)\n",
    "        recon_x, mu, logstd = model(data.x, data.edge_index)\n",
    "        recon_loss = F.mse_loss(recon_x, data.x)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logstd - mu.pow(2) - logstd.exp())\n",
    "        loss = recon_loss + kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print a small subset of the data being trained\n",
    "        if epoch % 50 == 0 and batch_idx == 0:  # Print for the first batch of every 50th epoch\n",
    "            print(f'Epoch: {epoch:03d}, Batch: {batch_idx:03d}')\n",
    "            print(f'Data being trained (first 5 rows):\\n{data.x[:5]}')\n",
    "            print(f'Reconstructed data (first 5 rows):\\n{recon_x[:5]}')\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Recon Loss: {recon_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, Total Loss: {loss.item():.4f}') \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconsruction errors on steady state data \n",
    "#Reading and Normalizing Features\n",
    "csv_file_path = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\steady_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features = pd.read_csv(csv_file_path, usecols=columns_to_read)\n",
    "features = features.iloc[0:50000].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "features1 = torch.tensor(features_normalized, dtype=torch.float)  # Use your actual features\n",
    "#features1 = torch.tensor(features,dtype=torch.float)\n",
    "\n",
    "x_test = features1\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "edge_index = positive_edge_index\n",
    "\n",
    "#features_tensor = features1\n",
    "#dataset = GraphDataset(features_tensor)\n",
    "#dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating recons errors with steady state data \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming model is your VGAE model and data is your Data object containing features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_features, _, _ = model(x_test, edge_index)\n",
    "\n",
    "# Compute MSE for each node feature\n",
    "mse_per_feature = F.mse_loss(recon_features, x_test, reduction='none')\n",
    "mse_per_node = torch.mean(mse_per_feature, dim=0)\n",
    "print(f'Mean Squared Error (MSE) per node feature:')\n",
    "for i, mse in enumerate(mse_per_node, start=1):\n",
    "    print(f'Node {i}: {mse.item()}')\n",
    "\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MSE values for each node\n",
    "#mse_values = [0.10052483528852463, 0.08730024099349976, 0.054440926760435104, \n",
    " #          0.1400376558303833, 0.2684721052646637, 0.10724300146102905,0.07387945801019669]\n",
    "mse_values = mse_per_node\n",
    "steady_values = mse_values\n",
    "#steady_values = [0.06045403331518173,0.04978080838918686,0.06117333471775055,0.03375500440597534,0.06546468287706375,0.06247369199991226,0.05447768419981003]\n",
    "\n",
    "# Node labels\n",
    "nodes = ['line 1', 'line 2', 'line 3', 'line 5', 'line 5', 'line 6', 'line S3I']\n",
    "\n",
    "# Calculate the mean of the MSE values\n",
    "#mean_mse = np.mean(steady_values)\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nodes, mse_values, marker='o', color='skyblue', label='steady')\n",
    "#plt.plot(nodes, steady_values, marker='o', color='blue', label='steady')\n",
    "\n",
    "# Add value labels on the points\n",
    "#for i, mse in enumerate(steady_values):\n",
    " #   plt.text(nodes[i], mse, round(mse, 4), ha='center', va='bottom')\n",
    "\n",
    "# Add a horizontal line for the mean MSE\n",
    "#plt.axhline(mean_mse, color='red', linewidth=2, linestyle='--', label=f' steady mean MSE: {mean_mse:.4f}')\n",
    "\n",
    "plt.xlabel('Lines')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Mean Squared Error (MSE) per Node Feature')\n",
    "plt.ylim(0, max(steady_values) * 7)  # Add some padding to the y-axis\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-4000: node 3\n",
    "4000-12000: node 4\n",
    "12008-13340: node 5\n",
    "13342-14600: node 1\n",
    "14676-16000: node 5\n",
    "16010-22600: node 2\n",
    "22680- 26680: node 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//proposed testing method \n",
    "\n",
    "test every 100 instances and count the frequency of most faulty line. The line with max frequency of faults will be the line experiencing the fault  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and Normalizing Features\n",
    "csv_file_path = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\fault_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features = pd.read_csv(csv_file_path, usecols=columns_to_read)\n",
    "features = features[0:26600]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "features1 = torch.tensor(features_normalized, dtype=torch.float)  # Use your actual features\n",
    "\n",
    "x_test = features1\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "edge_index = positive_edge_index\n",
    "\n",
    "#features_tensor = features1\n",
    "#dataset = GraphDataset(features_tensor)\n",
    "#dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "#calculating recons errors with steady state data \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming model is your VGAE model and data is your Data object containing features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_features, _, _ = model(x_test, edge_index)\n",
    "\n",
    "# Compute MSE for each node feature\n",
    "mse_per_feature = F.mse_loss(recon_features, x_test, reduction='none')\n",
    "mse_per_node = torch.mean(mse_per_feature, dim=0)\n",
    "print(f'Mean Squared Error (MSE) per node feature:')\n",
    "for i, mse in enumerate(mse_per_node, start=1):\n",
    "    print(f'Node {i}: {mse.item()}')\n",
    "    \n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MSE values for each node\n",
    "#mse_values = [0.10052483528852463, 0.08730024099349976, 0.054440926760435104, \n",
    " #          0.1400376558303833, 0.2684721052646637, 0.10724300146102905,0.07387945801019669]\n",
    "mse_values = mse_per_node\n",
    "steady_values = [0.07315168529748917,0.059811048209667206,0.07632388174533844,0.00765578355640173,0.08357910066843033,0.08549486100673676,0.06761068105697632]\n",
    "\n",
    "\n",
    "# Node labels\n",
    "nodes = ['line 1', 'line 2', 'line 3', 'line 4', 'line 5', 'line 6', 'line S3I']\n",
    "\n",
    "# Calculate the mean of the MSE values\n",
    "mean_mse = np.mean(steady_values)\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nodes, mse_values, marker='o', color='skyblue', label='fault')\n",
    "plt.plot(nodes, steady_values, marker='o', color='blue', label='steady')\n",
    "\n",
    "# Add value labels on the points\n",
    "for i, mse in enumerate(steady_values):\n",
    "    plt.text(nodes[i], mse, round(mse, 4), ha='center', va='bottom')\n",
    "\n",
    "# Add a horizontal line for the mean MSE\n",
    "plt.axhline(mean_mse, color='red', linewidth=2, linestyle='--', label=f' steady mean MSE: {mean_mse:.4f}')\n",
    "\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Mean Squared Error (MSE) per Node Feature')\n",
    "plt.ylim(0, max(steady_values) * 9.3)  # Add some padding to the y-axis\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#calculating recons errors with steady state data \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming model is your VGAE model and data is your Data object containing features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_features, _, _ = model(x_test, edge_index)\n",
    "\n",
    "# Compute MSE for each node feature\n",
    "mse_per_feature = F.mse_loss(recon_features, x_test, reduction='none')\n",
    "mse_per_node = torch.mean(mse_per_feature, dim=0)\n",
    "print(f'Mean Squared Error (MSE) per node feature:')\n",
    "for i, mse in enumerate(mse_per_node, start=1):\n",
    "    print(f'Node {i}: {mse.item()}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-4000: node 3\n",
    "4000-12000: node 4\n",
    "12008-13340: node 5\n",
    "13342-14600: node 1\n",
    "14676-16000: node 5\n",
    "16010-22600: node 2\n",
    "22680- 26680: node 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom testing function\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming your model and data (x_test, edge_index) are already defined\n",
    "#Reading and Normalizing Features\n",
    "csv_file_path = 'C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\fault_data.csv'\n",
    "columns_to_read = [4, 7, 13, 10, 19, 16, 22]\n",
    "features = pd.read_csv(csv_file_path, usecols=columns_to_read)\n",
    "features = features[16000:22600]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "features1 = torch.tensor(features_normalized, dtype=torch.float)  # Use your actual features\n",
    "\n",
    "x_test = features1\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "edge_index = positive_edge_index\n",
    "\n",
    "# Initialize a tensor to count the frequency of max MSE occurrences for each node\n",
    "\n",
    "num_nodes = x_test.size(1)\n",
    "max_mse_counts = torch.zeros(num_nodes, dtype=torch.int32)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Define start and end indices for the test range\n",
    "feature_start_idx = 0\n",
    "feature_end_idx = x_test.size(0)\n",
    "\n",
    "# Iterate over the test range in batches\n",
    "for i in range(feature_start_idx, feature_end_idx, batch_size):\n",
    "    # Get the current batch\n",
    "    batch_end_idx = min(i + batch_size, feature_end_idx)\n",
    "    batch_x_test = x_test[i:batch_end_idx]\n",
    "\n",
    "    # Perform forward pass to reconstruct features for the batch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon_features, _, _ = model(batch_x_test, edge_index)\n",
    "\n",
    "    # Compute MSE for each node feature in the batch\n",
    "    mse_per_feature = F.mse_loss(recon_features, batch_x_test, reduction='none')\n",
    "    mse_per_node = torch.mean(mse_per_feature, dim=0)  # Averaging over features\n",
    "\n",
    "    # Find the index (node) with the maximum MSE in this batch\n",
    "    max_mse_node = torch.argmax(mse_per_node).item()\n",
    "\n",
    "    # Increment the count for the node with max MSE\n",
    "    max_mse_counts[max_mse_node] += 1\n",
    "\n",
    "# Find the node with the highest frequency of max MSE\n",
    "most_frequent_max_mse_node = torch.argmax(max_mse_counts).item()+1\n",
    "\n",
    "# Print the node number with the highest frequency\n",
    "print(f'Node with the highest frequency of max MSE: Node {most_frequent_max_mse_node}')\n",
    "print(f'Frequency count for each node: {max_mse_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted mse\n",
    "\"\"\"dist_matrix = [[0,1,2,1,2,2,2],\n",
    "               [1,0,1,1,1,2,2],\n",
    "               [2,1,0,2,0,2,0],\n",
    "               [1,1,2,0,2,2,1],\n",
    "               [2,1,0,2,0,1,2],\n",
    "               [2,2,2,2,1,0,1],\n",
    "               [2,2,0,1,2,1,0]]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "furst find node with max mse \n",
    "find distance of every other node from that node \n",
    "then inverse the distances and multiply to the mse \n",
    "find sum of distance of all other nodes from that node then take average and ivnerse it then multiple it with mse of the reference node \n",
    "\n",
    "??statistical analysis??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find max, will be used to find the line in which maximum error is occuring for the given batch \n",
    "def max_freq (array = []):\n",
    "    for i in range (array.shape[0]):\n",
    "        max_index = 0\n",
    "        max_element = array[0]\n",
    "        if(array[i]> max_element):\n",
    "            max_element = array[i]\n",
    "            max_index = i        \n",
    "    return max_index+1\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_freq(mse_per_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom testing function \n",
    "feature_start_idx = 0\n",
    "feature_end_idx = 4000\n",
    "test_batch_size = 100\n",
    "array = torch.tensor([0,0,0,0,0,0,0])\n",
    "# array will contain count of each fault line at which fault occurs\n",
    "# ith index will contain frequency of occurence of (i+1)th line fault \n",
    "\n",
    "for i in range (int((feature_end_idx-feature_start_idx)/test_batch_size)): \n",
    "    # Compute MSE for each node feature\n",
    "    x_test = features1[feature_start_idx:feature_start_idx+test_batch_size]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "       recon_features, _, _ = model(x_test, edge_index)\n",
    "    mse_per_feature = F.mse_loss(recon_features, x_test, reduction='none')\n",
    "    mse_per_node = torch.mean(mse_per_feature, dim=0)\n",
    "    max_error_line = max_freq(mse_per_node) #finding which line had maximum mse\n",
    "    array[max_error_line-1] = array[max_error_line-1]+1  #updating the frequency of fault in that line \n",
    "    feature_start_idx = feature_start_idx + test_batch_size\n",
    "    \n",
    "\n",
    "#finding which line had maximum frequency of fault \n",
    "print(max_freq(array))    \n",
    "print(array)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_per_node.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming model is your VGAE model and data is your Data object containing features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_features, _, _ = model(x_test, edge_index)\n",
    "\n",
    "# Compute MSE for each node feature\n",
    "mse_per_feature = F.mse_loss(recon_features, x_test, reduction='none')\n",
    "mse_per_node = torch.mean(mse_per_feature, dim=0)\n",
    "print(f'Mean Squared Error (MSE) per node feature:')\n",
    "for i, mse in enumerate(mse_per_node, start=1):\n",
    "    print(f'Node {i}: {mse.item()}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.06045403331518173\n",
    "Node 2: 0.04978080838918686\n",
    "Node 3: 0.06117333471775055\n",
    "Node 4: 0.03375500440597534\n",
    "Node 5: 0.06546468287706375\n",
    "Node 6: 0.06247369199991226\n",
    "Node 7: 0.05447768419981003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MSE values for each node\n",
    "#mse_values = [0.10052483528852463, 0.08730024099349976, 0.054440926760435104, \n",
    " #          0.1400376558303833, 0.2684721052646637, 0.10724300146102905,0.07387945801019669]\n",
    "mse_values = mse_per_node\n",
    "steady_values = [0.06045403331518173,0.04978080838918686,0.06117333471775055,0.03375500440597534,0.06546468287706375,0.06247369199991226,0.05447768419981003]\n",
    "\n",
    "# Node labels\n",
    "nodes = ['Node 1', 'Node 2', 'Node 3', 'Node 4', 'Node 5', 'Node 6', 'Node 7']\n",
    "\n",
    "# Calculate the mean of the MSE values\n",
    "mean_mse = np.mean(steady_values)\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nodes, mse_values, marker='o', color='skyblue', label='fault')\n",
    "plt.plot(nodes, steady_values, marker='o', color='blue', label='steady')\n",
    "\n",
    "# Add value labels on the points\n",
    "for i, mse in enumerate(steady_values):\n",
    "    plt.text(nodes[i], mse, round(mse, 4), ha='center', va='bottom')\n",
    "\n",
    "# Add a horizontal line for the mean MSE\n",
    "plt.axhline(mean_mse, color='red', linewidth=2, linestyle='--', label=f' steady mean MSE: {mean_mse:.4f}')\n",
    "\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Mean Squared Error (MSE) per Node Feature')\n",
    "plt.ylim(0, max(steady_values) * 9.3)  # Add some padding to the y-axis\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
